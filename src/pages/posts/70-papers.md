---

layout: ../../layouts/MdPostDescLayout.astro
title: 'ML Papers'
LastUpdatedDate: 2024-04-16
CreatedDate: 2024-04-10
description: 'n/a'
author: 'Jeff'
tags: []


---
* [Main PA page](/posts/68-temp/)


### Papers from Seymour 4-10


1. [SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) 2024 P. Zhou, J. Pujara, et al.
1. [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200) 2024 X. Wang, Denny Zhou
1. [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) 2022 Yao, Zhao, ..., Cao
	* Question-Thought-Action-Observation
	* https://react-lm.github.io/
	* https://anonymous.4open.science/r/ReAct-2268/


#### Few-shot prompting with intermediate steps augmented demonstration exemplars
1. [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114) 2021 Maxwell Nye, Anders Johan Andreassen, et al.
1. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) 2022 J. Wei, X. Wang, ..., Quoc Le, Denny Zhou
1. [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks](https://arxiv.org/abs/2211.12588) 2022 Chen, Ma, Wang, Cohen
1. [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) 2022 Gao, Madaan, et al.
1. [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) 2023 Yao, Yu, ..., Narasimhan
1. [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625) 2023 Denny Zhou, N. SchÃ¤rli, et al.
	* Least-to-most: Q1 -> steps -> A1, A1/Q2 -> steps -> A2, few-shot

#### Zero-shot prompting with specific instructions which ask for showing certain intermediate steps
1. [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) 2022 Kojima et al.
1. [Large Language Models as Analogical Reasoners](https://arxiv.org/abs/2310.01714) 2023 Yasunaga et al.
