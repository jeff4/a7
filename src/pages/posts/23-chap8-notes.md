---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Goodfellow (GBC) Chap 8'
pubDate: 2023-11-15
LastUpdatedDate: 2023-12-28
CreatedDate: 2023-10-28
description: 'Selected notes from Chapter 8 (Optimization)'
author: 'Jeff'
tags: []
---

**Deep Learning (2016)** 
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville
* GBC

## Chapter 8: Optimization for Training Deep Models p. 267
1. Deep learning algorithims involve optimization in many contexts.
1. For example, performing inference in models such as PCA involves solving an optimization problem. 
1. We often use analytical optimization to write proofs or design algorithms.
1. Of all the many optimization problems involved in deep learning, ANN training is the most difficult.
	* It is quite common to invest days to months of time on hundreds of machines to solve even a single instance of the neural network training problem. 
	* Because this problem is so important and so expensive, a specialized set of optimization techniques have been developed for solving it. 
1. Chapter 8 presents the best optimization techniques for training ANNs.

### JH Break

1. We begin with a description of how optimization used as a training algorithm for a machine learning task diﬀers from pure optimization. 
1. Next, we present severalof the concrete challenges that make optimization of neural networks diﬃcult. 
1. We then deﬁne several practical algorithms, including both optimization algorithms themselves and strategies for initializing the parameters. 
1. More advanced algorithms adapt their learning rates during training or leverage information contained in the second derivatives of the cost function. 
1. Finally, we conclude with a review of several optimization strategies that are formed by combining simple optimizationalgorithms into higher-level procedures.

### 8.1 How Learning Differs from Pure Optimization

1. Optimization algorithms used for training of deep models diﬀer from traditional optimization algorithms in several ways. 
1. Machine learning usually acts indirectly.
1. In most machine learning scenarios, we care about some *performance measure* `P`, that is deﬁned with respect to the test set and may also be intractable. 
1. We therefore optimize `P` only indirectly. 
1. We reduce a diﬀerent cost function J(**&#952;**) inthe hope that doing so will improve `P`. 
1. This is in contrast to pure optimization, where minimizing J() is a goal in and of itself. 
1. Optimization algorithms for training deep models also typically include some specialization on the speciﬁc structure of machine learning objective functions. 

### JH Break

1. Typically, we can write the cost function J() as an average over teh training set where p<sub>data</sub> is the empirical distribution. I've rewritten Eq. 8.1 below.
	* J(**&#952;**) = **E<sub>(x&#8407;,y)</sub>** *times* **L [** f(x&#8407;,**&#952;**), y **]**
1. Additional notes on Equation 8.1 above. 
	* JH removed p<sub>data</sub> for clarity of reading. See p.268.
	* **L[]** is the per-example loss function aka the loss for each training example.
	* f(x&#8407;,**&#952;**) is the predicted output f() given input vector x&#8407;.
1. In the supervised learning case, y is the target output. 
1. Throughout Chapter 8, we develop the *unregularized supervised case*, where the inputs to function L[] are:
	* f(x&#8407;,**&#952;**)
	* y
1. It is trivial to extend this development to include **&#952;** and x&#8407; as additional input arguments to L[] for regularization. Or also to exclude y in the case of unsupervised learning.

### JH Break

1. Equation 8.2 is exactly the same except it denotes **J<sup>*</sup>** which is distinct from J, except **J<sup>*</sup>** tries to minimize the objective function across expectation **E** across the *entire data-generating distribution** p<sub>data</sub> rather than just the p<sub>data</sub> generated by the **finite training set**. 
1. Thus, Eq 8.2 has a vanilla p<sub>data</sub> whereas original Eq. 8.1 has a p-hat based on the finite training set.

### 8.1.1 Empirical Risk Minimization

1. The goal of a machine learning algorithm is to reduce the expected generalizationi error given by equation 8.2. 
1. This quantity is known as the risk. 
1. We emphasize here that the expectation is taken over the true underlying distribution p<sub>data</sub>. 

### Notes from December 2023
1. GBC Chapter 8, section 8.3 is on Stochastic Gradient Descent (SGD)
1. AdaGrad and then in section 8.5.3 Adam adaptive learning rate optimization
	* Batch Normalization is extremely exciting circa 2015 (section 8.7.1) * section 8.7.4 Supervised Pretraining

##### Double-struck R for Real numbers
* "rp = double-struck R = &#8477;
* "tp = theta = **&#952;**
* "hp = theta-hat = **&#952;-hat**
* "pp = vector arrow above prior character = &#8407;
* "yp = lowercase phi = &#966;
* "ip = infinity = &#8734;
* "ap = rightward pointing arrow = &#8594;
* "ep = element of = &#8712;
* "wp = matrix W = **`W`**
* "sp = lowercase sigma for sigmoid function &#963;
* "np = nabla aka gradient upside down triangle &#8711;
* "op = omega = &#937;
* "jp = Capital letter J function with a tilde above it = J&#771;

