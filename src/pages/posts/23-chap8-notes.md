---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Goodfellow (GBC) Chap 8'
pubDate: 2023-11-15
LastUpdatedDate: 2023-12-28
CreatedDate: 2023-10-28
description: 'Selected notes from Chapter 8 (Optimization)'
author: 'Jeff'
tags: []
---

**Deep Learning (2016)** 
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville
* GBC

## Chapter 8: Optimization for Training Deep Models p. 267
1. Deep learning algorithims involve optimization in many contexts.
1. For example, performing inference in models such as PCA involves solving an optimization problem. 
1. We often use analytical optimization to write proofs or design algorithms.
1. Of all the many optimization problems involved in deep learning, ANN training is the most difficult.
	* It is quite common to invest days to months of time on hundreds of machines to solve even a single instance of the neural network training problem. 
	* Because this problem is so important and so expensive, a specialized set of optimization techniques have been developed for solving it. 
1. Chapter 8 presents the best optimization techniques for training ANNs.

### 01 JH Break

1. We begin with a description of how optimization used as a training algorithm for a machine learning task diﬀers from pure optimization. 
1. Next, we present severalof the concrete challenges that make optimization of neural networks diﬃcult. 
1. We then deﬁne several practical algorithms, including both optimization algorithms themselves and strategies for initializing the parameters. 
1. More advanced algorithms adapt their learning rates during training or leverage information contained in the second derivatives of the cost function. 
1. Finally, we conclude with a review of several optimization strategies that are formed by combining simple optimizationalgorithms into higher-level procedures.

### 8.1 How Learning Differs from Pure Optimization

1. Optimization algorithms used for training of deep models diﬀer from traditional optimization algorithms in several ways. 
1. Machine learning usually acts indirectly.
1. In most machine learning scenarios, we care about some *performance measure* `P`, that is deﬁned with respect to the test set and may also be intractable. 
1. We therefore optimize `P` only indirectly. 
1. We reduce a diﬀerent cost function J(**&#952;**) inthe hope that doing so will improve `P`. 
1. This is in contrast to pure optimization, where minimizing J() is a goal in and of itself. 
1. Optimization algorithms for training deep models also typically include some specialization on the speciﬁc structure of machine learning objective functions. 

### 02 JH Break

1. Typically, we can write the cost function J() as an average over teh training set where p<sub>data</sub> is the empirical distribution. I've rewritten Eq. 8.1 below.
	* J(**&#952;**) = **E<sub>(x&#8407;,y)</sub>** *times* **L [** f(x&#8407;,**&#952;**), y **]**
1. Additional notes on Equation 8.1 above. 
	* JH removed p<sub>data</sub> for clarity of reading. See p.268.
	* **L[]** is the per-example loss function aka the loss for each training example.
	* f(x&#8407;,**&#952;**) is the predicted output f() given input vector x&#8407;.
1. In the supervised learning case, y is the target output. 
1. Throughout Chapter 8, we develop the *unregularized supervised case*, where the inputs to function L[] are:
	* f(x&#8407;,**&#952;**)
	* y
1. It is trivial to extend this development to include **&#952;** and x&#8407; as additional input arguments to L[] for regularization. Or also to exclude y in the case of unsupervised learning.

### 03 JH Break

1. Equation 8.2 is exactly the same except it denotes **J<sup>*</sup>** which is distinct from J, except **J<sup>*</sup>** tries to minimize the objective function across expectation **E** across the *entire data-generating distribution** p<sub>data</sub> rather than just the p<sub>data</sub> generated by the **finite training set**. 
1. Thus, Eq 8.2 has a vanilla p<sub>data</sub> whereas original Eq. 8.1 has a p-hat based on the finite training set.

### 8.1.1 Empirical Risk Minimization

1. The goal of a machine learning algorithm is to reduce the expected generalizationi error given by equation 8.2. 
1. This quantity is known as the risk. 
1. We emphasize here that the expectation is taken over the true underlying distribution p<sub>data</sub>. 
1. If we knew the true distribution p<sub>data</sub>(x&#8407;, y), then the risk minimization would be an optimization task solvable by a vanilla optimization problem. 
1. However, when we do *not* know the true distribution p<sub>data</sub>(x&#8407;, y), and only have access to the distribution from a set of training samples, then we have an ML problem.

### 04 JH Break

1. The simplest way to convert an ML problem back into an optimization problem is to minimize the expected loss on the training set. 
1. In other words, we replace the true distribution p(x&#8407;, y) with the training set's empirical distribution p-hat(x&#8407;, y)
1. Equation 8.3 on p. 268 shows the *empirical risk* **E**<sub>x&#8407;, y ~ **p-hat**<sub>data</sub>(x&#8407;, y)</sub> which needs to be minimized.
1. This process is called *empirical risk minimization*.

### 05 JH Break

1. In this context, ML is still very similar to straightforward optimization.
1. Rather than optimizing the risk directly, we optimize the empirical risk and hope that the risk decreases signiﬁcantly as well. 
1. A variety of theoretical results establish conditions under which the true risk can be expected to decrease by various amounts.
1. Nonetheless, empirical risk minimization is prone to overﬁtting. 
1. Models with high capacity can simply memorize the training set. 
1. In many cases, empirical risk minimization is not really feasible. 

### 06 JH Break

1. The most eﬀective modern optimization algorithms are based on gradient descent. 
1. However, many useful loss functions, such as 0-1 loss, have no useful derivatives (i.e., the derivative is either zero or undeﬁned everywhere). 
1. These two problems mean that, in the context of deep learning, we rarely use empirical risk minimization. 
1. Instead, we must use a slightly diﬀerent approach, in which the quantity that we actually optimize is even more diﬀerentfrom the quantity that we truly want to optimize. 

### 8.1.2 Surrogate Loss Functions and Early Stopping

1. Sometimes, the loss function we actually care about (say, classiﬁcation error) is not one that can be optimized eﬃciently. 
1. For example, trying to minimize `expected 0-1 loss` to an exact level is is typically intractable even for a linear classiﬁer. (see p. 269 for more details)
1. In such situations, one typically optimizes a surrogate loss function instead.
1. Although this is surrogate function is just a proxy, this approach has some advantages.
1. For example, the negative log-likelihood of the correct class is typically used as a surrogate for the 0-1 loss. 
1. The negative log-likelihood allows the model to estimate the conditional probability of the classes, given the input, and if the model can do that well, then it can pick the classes that yield the least classiﬁcation error inexpectation.

### 07 JH Break

1. In some cases, a surrogate loss function actually results in being able to learn more. (I edited this out, refer to p. 269-270 to see an example of this.)
1. A very important diﬀerence between optimization in general and optimization used for ML training algos is that ML training algorithms do not usually halt at a local minimum. 
1. Instead, an ML algo usually minimizes a surrogate loss function but *halts* when a convergence criterion based on early stopping (see Section 7.8) is satisﬁed. 
1. Typically the early stopping criterion is based on the true underlying loss function, such as 0-1 loss measured on a validation set, and is designed to cause the algorithm to halt whenever overﬁtting begins to occur.
1. Training often halts while the surrogate loss function still has large derivatives, which is very diﬀerent from the pure optimization setting, where an optimization algorithm is considered to have converged when the gradient becomes very small.

### 8.1.3 Batch and Minibatch Algorithms
1. One aspect of machine learning algorithms that separates them from general optimization algorithms is that the objective function usually decomposes as a sum over the training examples. 
1. Optimization algorithms for machine learning typically compute each update to the parameters based on an expected value of the cost function estimated using only a subset of the terms of the full cost function. i
1. For example, maximum likelihood estimation problems, when viewed in log space, decompose into a sum over each example. (See Equation 8.4).

### 08 JH Break

1. Skipped some stuff from p. 270 - p. 271. 
1. Optimization algorithms that use the entire training set are called **batch** or **deterministic gradient methods**, because they process all the training examples simultaneously in a large batch. 
1. This terminology can be somewhat confusing because the word “batch” is also often used to describe the *mini-batch* used by **mini-batch stochastic gradient descent**. 
1. Typically the term “batch gradient descent” implies the use of the full training set, while the use of the term “batch” to describe a group of examples does not. 
1. For example, it is common to use the term “batch size” to describe the size of a minibatch.

### 09 JH Break

1. Optimization algorithms that use only a single example at a time are sometimes called *stochastic methods* and sometimes called *online methods*. 
1. The term “online” is usually reserved for when the examples are drawn from a stream of continually created examples rather than from a ﬁxed-size training set over which several passes are made.
1. Most algorithms used for deep learning fall somewhere in between the two extremes.
1. These algos use more than one example at a time but fewer than all training examples at once.
1. These were traditionally called *minibatch* or *minibatch stochastic* methods; it is now common to simply call them *stochastic methods*.
1. **The canonical example of a stochastic method is stochastic gradient descent (SGD).**
1. See Section 8.3.1 for in-depth coverage of SGD.
1. See p.272 for list of factors that help determine how to adjust minibatch size for ML purposes.

### 10 JH Break

1. p. 273 - It is also crucial that the minibatches be selected randomly. 
1. Computing an unbiased estimate of the expected gradient from a set of samples requires that those samples be independent. 
1. We also wish for two subsequent gradient estimates to be independent from each other, so two subsequent minibatches of examples should also be independent from each other. 
1. Many datasets are most naturally arranged in a way where successive examples are highly correlated.

### 11 JH Break

1. Thus, when the order of the examples in the training dataset have some trend, it is necessary to shuffle the examples before selecting minibatches. 
1. For example of long list of blood test samples and results and very large datasets, go to p. 273 for more discussion on how to manage the need for randomization.
1. Skipped a bunch of interesting material from "An interesting motivation for minibatch stochastic gradient descent..." at the end of p. 273 to beginning of p.275.

### 12 JH Break

1. Some datasets have been growing rapidly in size and even faster than available computing power. 
1. Thus it is becoming more common for ML apps to use each training example *only once* or even to make an incomplete pass through the training set. 
1. When using an extremely large training set, overﬁtting is not an issue. 
1. In these cases, underfitting and computational eﬃciency become the bigger concerns. 

### 8.2 Challenges in Neural Network Optimization
1. Optimization in general is an extremely diﬃcult task. 
1. Traditionally, ML has avoided the diﬃculty of general optimization by carefully designing the **objective function J()** and related constraints to ensure that the optimization problem is convex. 
1. However, when training ANNs specifically,we must confront general non-convex case. Because non-convex situations are unavoidable for ANNs vs. traditional and simple ML.
1. Even convex optimization is not without its complications. 
1. In this section 8.2, we summarize several of the most prominent challenges involved in optimization for training deep models.

### 8.2.1 Ill-Conditioning

1. Some challenges arise even when optimizing convex functions. 
1. Of these, the most prominent is ill-conditioning of the *Hessian matrix* **H**. 
1. This is a very general problem in most numerical optimization, convex or otherwise, and is described in more detail in section 4.3.1.
1. The ill-conditioning problem is generally believed to be present in neuralnetwork training problems. 
1. Ill-conditioning can manifest by causing SGD to "get stuck” in the sense that even very small steps increase the cost function.

### 8.2.2 Local Minima

1. The point of a convex function is that any local minimum is guaranteed to be a global minimum.
1. Some convex functions have a flat region at the bottom. This means that multiple points all satisfy the global minimum. 
1. But any of the points in that flat region are acceptable solutions.
1. When optimizing a convex function, we know that we have reached a good solution if we find a critical point of any kind.

### 18 JH Break

1. However, ANNs are non-convex vunctions; thus there are many local minina which are not guaranteed to be global mininum.
1. Indeed, nearly any deep model is essentially guaranteed to have an extremely large number of local minima. 
1. As we will see, however, this is not necessarily a major problem.
1. Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the **model identiﬁability problem**. 
1. A model is said to be identiﬁable if a suﬃciently large training set can rule out all but one setting of the model’s parameters. 
1. Models with latent variables are often not identiﬁable because we can obtain equivalent models by exchanging latent variables with each other. 

### 19 JH Break

1. For example, we could take a neural network and modify **layer 1** by swapping the incoming weight vector for **unit vector i** with the incoming weight vector for **unit vector j**, then do the same for the outgoing weight vectors. 
1. If we have *m* layers with *n* units each, then there are **n!<sup>m</sup>** ways of arranging the hidden units. 
1. This kind of nonidentiﬁability is known as weight space symmetry.
1. In addition to weight space symmetry, many kinds of neural networks have additional causes of nonidentiﬁability. 

### 20 JH Break

1. For example, in any **rectiﬁed linear** or **maxout** network, we can scale all the incoming weights and biases of a unit by alpha if we also scale all its outgoing weights by `1/alpha`. 
1. This means that—if the cost function does not include terms such as weight decay that depend directly on the weights rather than the models’ outputs—every local minimum of a rectiﬁed linear or maxout network lies on an (*m × n*)-dimensional hyperbola of equivalent local minima.
1. These **model identiﬁability issues** mean that a neural network cost function can have an extremely large or even uncountably inﬁnite amount of local minima.
1. However, all these local minima arising from nonidentiﬁability are equivalent to each other in cost function value. 

### 21 JH Break

1. As a result, these local minima are not a problematic form of nonconvexity.
1. Local minima can be problematic if they have high cost in comparison to the global minimum.
1. For a long time, ML practitioners believed that local minima were a common problem plaguing ANN optimization.
1. However as of 2016, this is not as much of a concern.
1. Experts now suspect that, for sufficiently large ANNs, =most local minima have a low **cost function** value. 
	* It is not that important to find a true global minimum rather than to find a point in parameter space that is 'low enough' but not the absolute global minimum.

### 8.2.3 Plateaus, Saddle Points, and Other Flat Regions

1. Another type of critical point is the **saddle point**.
1. A saddle point is surounded by points with greater costs in some dimensions, and points with lower costs in other dimensions. 
1. In fact, for many high-dimensional non-convex functions, local minima and maxima are relatively rare compared to saddle points.
1. At a saddle point, the Hessian matrix has both positive and negative eigenvalues.
	* Points lying along eigenvectors with positive eigenvalues have a higher cost than the saddle point.
	* Points lying along eigenvectors with negative eigenvalues have a lower cost than the saddle point.
1. We can think of a saddle point as a being a local minimum along one cross-section of the cost function and a local maximum along another cross-section.
1. Classic picture of saddle point Fig 4.5 p. 86 in Chap 4. 

### 22 JH Break

1. Saddle points are a problem for Newton's Method. p.279  
	* Gradient descent is designed to move “downhill” and is not explicitly designed to seek a critical point. 
	* Newton’s method, however, is designed to solve for a point where the gradient is zero. 
	* Without appropriate modiﬁcation, it can jump to a saddle point. 
1. The proliferation of saddle points in high-dimensional spaces presumably explains why second-order methods have *not* succeeded in replacing gradient descent for neural network training. 
1. Dauphin et al. (2014) introduced a saddle-free Newton method for second-order optimization and showed that it improves signiﬁcantly over the traditional version. 
1. Second-order methods remain diﬃcult to scale to large neural networks, but this saddle-free approach holds promise if it can be scaled.

### 8.2.4 Cliffs and Exploding Gradients

### 8.2.5 Long-Term Dependencies

### 8.2.6 Inexact Gradients

### 8.2.7 Poor Correspondence between Local and Global Structure

### 8.2.8 Theoretical Limits of Optimization

### 8.3.1 Stochastic Gradient Descent

### 8.3.2 Momentum

### 8.3.3 Nesterov Momentum

### 8.4 Parameter Initialization Strategies
* p. 292 - 298

### 8.5 Algorithms with Adaptive Learning Rates
* 8.5.1 AdaGrad
* 8.5.2 RMSProp
* 8.5.3 Adam

















### Notes from December 2023
1. GBC Chapter 8, section 8.3 is on Stochastic Gradient Descent (SGD)
1. AdaGrad and then in section 8.5.3 Adam adaptive learning rate optimization
	* Batch Normalization is extremely exciting circa 2015 (section 8.7.1) 
	* Section 8.7.4 Supervised Pretraining

##### Double-struck R for Real numbers
* "rp = double-struck R = &#8477;
* "tp = theta = **&#952;**
* "hp = theta-hat = **&#952;-hat**
* "pp = vector arrow above prior character = &#8407;
* "yp = lowercase phi = &#966;
* "ip = infinity = &#8734;
* "ap = rightward pointing arrow = &#8594;
* "ep = element of = &#8712;
* "wp = matrix W = **`W`**
* "sp = lowercase sigma for sigmoid function &#963;
* "np = nabla aka gradient upside down triangle &#8711;
* "op = omega = &#937;
* "jp = Capital letter J function with a tilde above it = J&#771;
* "dp = p<sub>data</sub>


p<sub>data</sub>
