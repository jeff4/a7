---
layout: ../../layouts/MdPostDescLayout.astro
title: 'Conclusion'
LastUpdatedDate: 2024-01-10
CreatedDate: 2023-12-25
description: 'Matteo Pasquinelli'
author: 'Jeff'
tags: []


---
## The Automation of General Intelligence
### Quotes

### Section 1 
1. There will be a day in the future when current AI will be considered an archaism, one technical fossil to study among others. 
1. In the passage from *Capital* quoted above, Marx suggested a similar analogy that resonates with today's science and technology studies: in the same way in which fossil bones disclose the nature of ancient species and the ecosystems in which they lived, similarly, technical artefacts reveal the form of the society that surrounds and runs them.
1. The analogy is relevant, I think, for all machines and also for machine learning, whose abstract models do in reality encode a concretion of social relations and collective behaviours, as this book has tried to demonstrate in reformulating the nineteenth-century labour theory of automation for the age of AI.  
1. This book began with a simple question: What relation exists between labour, rules, and automation, i.e., the invention of new technologies? 
1. To answer this question, it has illuminated practices, machines, and algorithms from different perspectives -- from the 'concrete' dimension of production and the 'abstract' dimension of disciplines such as mathematics and computer science. 
1. The concern, however, has not been to repeat the separation of the concrete and abstract domains but to see their coevolution throughout history: eventually to investigate labour, rules, and automation, dialectically, as *material abstractions*. 
1. Chapter 1 emphasised this aspect by highlighting how ancient rituals, counting tools, and 'social algorithms' all contributed to the making of mathematical ideas. To affirm, as did the introduction, that *labour is a logical activity* is not a way of abdicating to the mentality of industrial machines and corporate algorithms, but rather of recognising that human praxis expresses its own logic (an *anti-logic*, some might say) -- a power of speculation and invention, before technoscience captures and alienates it.
1. The thesis that labour has to become 'mechanical' on its own, before machinery replaces it, is an old fundamental principle that has simply been forgotten. 
1. As illustrated in Part I, it dates back at least to Adam Smith's exposition in *The Wealth of Nations* (1776), which Hegel also commented upon already in his Jena lectures (1805--06). 
1. Hegel's notion of *abstract labour*, as labour that gives *form* to machinery, was already indebted to British political economy before Marx contributed his own radical critique of the concept. 
1. As seen in chapter 2, however, it fell to Charles Babbage to systematise Adam Smith's insight in a consistent *labour theory of automation*. 
1. Babbage complemented this theory with the *principle of labour calculation* (known since then as the 'Babbage principle') to indicate that the division of labour also allows the precise computation of labour costs. 
1. Part I of this book can be considered an exegesis of Babbage's two *principles of labour analysis* and their influence on the common history of political economy, automated computation, and machine intelligence. 
1. Although it may sound anachronistic, Marx's theory of automation and relative surplus-value extraction share common postulates with the first projects of machine intelligence.  
1. Marx overturned the industrialist perspective -- 'the eye of the master' -- that was inherent in Babbage's principles. 
1. In *Capital*, he argued that the *social relations of production* (the division of labour within the wage system) drive the development of the *means of production* (tooling machines, steam engines, etc.) and not the other way around, as technodeterministic readings have been claiming then and now by centring the Industrial Revolution around technological innovation only. 
1. Of these principles of labour analysis Marx made also something else: he considered the cooperation of labour not only as a principle to explain the design of machines but also to define the political centrality of what he called the *Gesamtarbeiter*, the *general worker*. 
1. The figure of the general worker was a way of acknowledging the machinic dimension of living labour and confronting the 'vast automaton' of the industrial factory on the same scale of complexity. 
1. Eventually, it was also a necessary figure to ground, on a more solid politics, the ambivalent idea of the *general intellect* that Ricardian socialists such as William Thompson and Thomas Hodgskin pursued, as seen in chapter 4.

### From the assembly lines to pattern recognition

1. This book has provided an expanding history of the division of labour and its metrics as a way to identify the operative principle of AI in the long run. 
1. As we have seen, at the turn of the nineteenth century, the more the division of labour extended into a globalised world, the more troublesome its management became, requiring new techniques of communication, control, and 'intelligence'. 
1. While, within the manufactory, labour management could be still sketched in a simple flow chart and measured by a clock, it was highly complicated to visualise and quantify what Émile Durkheim, already in 1893, defined as 'the division of social labour'. 
1. The 'intelligence' of the factory's master could no longer survey the entire production process in a single glance; now, only the infrastructures of communication could achieve this role of supervision and quantification. 
1. New mass media, such as the telegraph, telephone, radio, and television networks made possible communication across countries and continents, but they also opened up new perspectives on society and collective behaviours. 
1. As seen in chapter 5, James Beniger aptly described the rise of information technologies as a 'control revolution' that proved necessary in that period for governing the economic boom and commercial surplus of the Global North. 
1. After World War II, the control of this extended logistics became the concern of a new discipline of the military that bridged mathematics and management: operations research. 
1. However, it should be considered that also the transformations of the working class within and across countries, marked by cycles of urban conflicts and decolonial struggles, were among the factors that prompted the rise of these new technologies of control. 
1. Chapter 6 endeavoured to trace the historical coincidence between cybernetic projects of self-organisation and the social drives to self-organisation after World War II, as exemplified by countercultural and anti-authoritarian movements.  
1. The scale shift of labour composition from the nineteenth to the twentieth centuries affected also the *logic of automation*, that is, the scientific paradigms involved in this transformation. 
1. The relatively simple industrial division of labour and its seemingly rectilinear assembly lines could easily be compared to a simple *algorithm*, a rulebased procedure with an 'if/then' structure which has its equivalent in the logical form of *deduction*. 
1. Deduction, not by coincidence, is the logical form that via Leibniz, Babbage, Shannon, and Turing innervated into electromechanical computation and eventually symbolic AI. 
1. Deductive logic is useful for modelling simple processes, but not systems with a multitude of autonomous agents, such as society, the market, or the brain. 
1. In these cases, deductive logic is inadequate because it would explode any procedure, machine, or algorithm into an exponential number of instructions (see chapter 7). 
1. Out of similar concerns, cybernetics started to investigate self- organisation in living beings and machines to simulate order into high- complexity systems that could not be easily organised according to hierarchical and centralised methods. 
1. This was fundamentally the rationale behind connectionism and artificial neural networks (as discussed in chapter 9) and also early research on distributed networks of communication such as Arpanet (the progenitor of the internet).  
1. Across the twentieth century, many other disciplines recorded the growing complexity of social relations. 
1. The twin concepts of *Gestalt* and *pattern*, for instance, as employed respectively by Kurt Lewin and Friedrich Hayek and described earlier in this book, were an example of how psychology and economics responded to a new composition of society. 
1. Lewin introduced holistic notions such as *force field* and *hodological space* to map group dynamics at different scales between the individual and the mass society. 
1. Meanwhile, as we saw in chapter 8, Hayek hijacked the disparate notion of pattern in order to sketch a theory of the market and the mind based on radical individualism.  
1. French thought has been particularly fertile and progressive in this direction. 
1. The philosophers Gaston Bachelard and Henri Lefebvre proposed, for example, the method of *rhythmanalysis*, as a study of social rhythms in the urban space (which Lefebvre described according to the four typologies of arrhythmia, polyrhythmia, eurhythmia, and isorhythmia). 
1. In a similar way, French archaeology engaged with the study of expanded forms of social behaviour in ancient civilisations. 
1. For instance, the paleoanthropologist André Leroi-Gourhan, together with others, introduced the idea of the operational chain (*chaîne opératoire*) to explain the way pre- historic humans produced utensils. 
1. At the culmination of this long tradition of *diagrammatisation* of social behaviours in French thought, Gilles Deleuze wrote his famous 'Postscript on the Society of Control', which declared that power was no longer concerned with the discipline of individuals but with the control of *dividuals*, that is of the fragments of an extended and deconstructed body.  
1. Lewin's force fields, Lefebvre's urban rhythms, and Deleuze's dividuals can be seen as predictions of the principles of *algorithmic governance* which have been established with the network society and its vast data centres since the late 1990s. 
1. The 1998 launch of Google's PageRank algorithm -- a method for organising and searching the chaotic hypertext of the web -- is considered, by convention, the first large-scale elaboration of 'big data' from digital networks. 
1. These techniques for network mapping have become nowadays ubiquitous: Facebook, for instance, uses the Open Graph protocol to quantify the networks of human relations that feed the attention economy of its platform. 
1. The US military has been using its own controversial techniques of *pattern-of-life* analysis to map social networks in war zones and to identify targets of drone strikes which, as known, have killed innocent civilians. 
1. More recently, gig economy platforms and logistics giants such as Uber, Deliveroo, Wolt, and Amazon started to trace their fleet of riders and drivers via geolocation apps. 
1. All these techniques are part of the new field of 'people analytics' (also known as 'social physics' or 'psychographics'), which is but the application of statistics, data analytics, and machine learning to the problem of labour power in post-industrial society.

### The automation of psychometrics, or general intelligence

1. If attractive concepts such as 'pattern', 'Gestalt', or 'model' are not situated in an economic perspective (an opportunity Hayek, for instance, did not miss), their use may easily turn into a self-referential culturalist exercise. 
1. The division of labour as much as the design of machines and algorithms are not abstract forms per se but means for measuring labour and social behaviours and discriminating people according to their productive capacity. 
1. As the Babbage principles outlined in chapter 2 indicate, any division of labour entails a metrics: a measurement of workers' performativity and efficiency, but also a judgement about classes of skill, which involves an implicit social hierarchy. 
1. Metrics of labour were introduced to assess what is and is not productive, to manipulate a social asymmetry while declaring an equivalence to the money system. 
1. During the modern age, factories, barracks, and hospitals have pursued a discipline and organisation of bodies and minds with similar methods, as Michel Foucault sensed among others.  
1. At the end of the nineteenth century, the metrology of labour and behaviours found an ally in a new field of statistics: psychometrics. 
1. Psychometrics had the purpose of measuring the skills of the population in resolving basic tasks, making statistical comparisons on cognitive tests rather than taking measurements of physical performance as in the earlier field of psychophysics. 
1. As part of the controversial legacy of Alfred Binet, Charles Spearman, and Louis Thurstone, psychometrics can be considered one of the main genealogies of statistics, which has never been a neutral discipline so much as one concerned with the 'measure of man', the institutions of norms of behaviour, and the repression of abnormalities. 
1. The transformation of the metrics of labour into the *psychometrics of labour* is a key passage for both management and technological development in the twentieth century. 
1. It is telling, as we saw in chapter 9, that in designing the first artificial neural network perceptron Frank Rosenblatt was not only inspired by theories of neuroplasticity but also by tools of multivariable analysis that psychometrics imported into US psychology in the 1950s.  
1. From this perspective, this book attempted to clarify how the project of AI has actually emerged from the automation of the psychometrics of labour and social behaviours rather than the quest to solve the 'enigma' of intelligence. 
1. In a concise summary of the history of AI, one could say that the mechanisation of the 'general intellect' of the industrial age into the 'artificial intelligence' of the twenty-first century was made possible thanks to the statistical measurement of skill, such as Spearman's 'general intelligence' factor and its subsequent automation into artificial neural networks. 
1. If in the industrial age the machine was considered as an embodiment of science, knowledge, and the 'general intellect' of workers, in the information age artificial neural networks became the first machine to encode 'general intelligence' into statistical tools -- at the beginning, specifically, to automate pattern recognition as one of the key tasks of 'artificial intelligence'. 
1. In short, the current form of AI, machine learning, is the automation of the statistical metrics which were originally introduced to quantify cognitive, social, and work-related abilities. 
1. The application of psychometrics through information technologies is not a phenomenon unique to machine learning.
1. The 2018 Facebook--Cambridge Analytica data scandal, in which the consulting firm was enabled to collect the personal data of millions without their consent, is a reminder of how large-scale psychometrics is still used by corporate and state actors in the attempt to predict and manipulate collective behaviours.  
1. Given their legacy in the statistical tools of nineteenth-century biometrics, it is also not surprising that deep artificial neural networks have recently unfolded into advanced techniques of surveillance, such as facial recognition and pattern-of-life analysis. 
1. Critical AI scholars such as Ruha Benjamin and Wendy Chun, among others, have exposed the racist origins of these techniques of identification and profiling that, like psychometrics, almost represent technical proof of the social bias of AI. 
1. They have rightly identified the power of discrimination at the core of machine learning, and how this aligns it with the apparatuses of normativity of the modern age, including the questionable taxonomies of medicine, psychiatry, and criminal law.  
1. The metrology of intelligence pioneered in the late nineteenth century, with its implicit and explicit agenda of social and racial segregation, still operates at the core of AI to discipline labour and replicate productive hierarchies of knowledge. 
1. The rationale of AI is therefore not only the automation of labour but the reinforcement of these social hierarchies in an indirect way. 
1. By implicitly declaring what can be automated and what cannot, AI has imposed a new metrics of intelligence at each stage of its development. 
1. But to compare human and machine intelligence implies also a judgement about which human behaviour or social group is more intelligent than another, which workers can be replaced and which cannot. 
1. Ultimately, AI is not only a tool for automating labour but also for imposing standards of *mechanical intelligence* that propagate, more or less invisibly, social hierarchies of knowledge and skill. As with any previous form of automation, AI does not simply replace workers but displaces and restructures them into a new social order.

### The automation of automation

1. Looking carefully at how statistical tools that were conceived to rate cognitive skills and discriminate between people's productivity turned into algorithms, a more profound aspect of automation becomes apparent. 
1. In fact, the study of the metrology of labour and behaviours reveals that automation emerges in some cases from the transformation of the measurement instruments themselves into kinetic technologies. 
1. Tools for labour quantification and social discrimination have become 'robots' in their own right. 
1. Before psychometrics, one could refer to how the clock used to measure labour time in the factory was later implemented by Babbage for the automation of mental labour in the Difference Engine (see chapter 2). 
1. Cyberneticians such as Norbert Wiener still considered the clock as a key model for both the brain and the computer. 
1. In this respect, the historian of science Henning Schmidgen has noted how the chronometry of nervous stimuli contributed to the consolidation of brain metrology and also McCulloch and Pitts's model of neural networks. 
1. The theory of automation which this book has illustrated, then, does not point only to the emergence of machines from the logic of labour management but also from the instruments and metrics for quantifying human life in general and making it productive.  
1. This book has sought to show that AI is the culmination of the long evolution of labour automation and quantification of society. 
1. The statistical models of machine learning do not appear, in fact, to be radically different but rather homologous to the design of industrial machines: they are indeed constituted by the same analytical intelligence of tasks and collective behaviours, albeit with a higher degree of complexity (i.e., number of parameters). 
1. Like industrial machines whose design gradually emerged through routine tasks and trial-and-error adjustments, machine learning algorithms adapt their internal model to the patterns in the training data through a comparable trial-and-error process. 
1. The *design* of a machine as well as the *model* of a statistical algorithm can be said to follow a similar logic: both are based on the imitation of an external configuration of space, time, relations, and operations. 
1. In the history of AI, this was as true of Rosenblatt's perceptron (which aimed to record the gaze's movements and spatial relations of the visual field) as of any other machine learning algorithm nowadays (e.g., support vector machines, Bayesian networks, transformer models).  
1. Whereas the industrial machine embodies the diagram of the division of labour in a determined way (think of the components and limited 'degrees of freedom' of a textile loom, a lathe, or a mining excavator), machine learning algorithms (especially recent AI models with a vast numbers of parameters) can imitate complex human activities. 
1. Although with problematic levels of approximation and bias, a machine learning model is an adaptive artefact that can encode and reproduce the most diverse configurations of tasks. 
1. For example, one and the same machine learning model can emulate the movement of robotic arms in assembly lines as much as the driver's operations in a self-driving car; the same model can also translate between languages as much as describe images with colloquial words.  
1. The rise of large foundation models in recent years (e.g., BERT, GPT, CLIP, Codex) demonstrates how one single deep learning algorithm can be trained on one vast integrated dataset (comprising text, images, speech, structured data, and 3-D signals) and used to automate a wide range of so- called downstream tasks (question answering, sentiment analysis, information extraction, text generation, image captioning, image generation, style transfer, object recognition, instruction following, etc.). 
1. For the way in which they have been built on large repositories of cultural heritage, collective knowledge, and social data, large foundation models are the closest approximation of the mechanisation of the 'general intellect' which was envisioned in the industrial age. 
1. An important aspect of machine learning that foundation models demonstrate is that the automation of individual tasks, the codification of cultural heritage, and the analysis of social behaviours have no technical distinction: they can be performed by the one and same process of statistical modelling.  
1. In conclusion, machine learning can be seen as the project to automate the very process of machine design and model making -- which is to say, the automation of the labour theory of automation itself. 
1. In this sense, machine learning and, specifically, large foundation models represent a new definition of the Universal Machine, for their capacity is not just to perform computational tasks but to imitate labour and collective behaviours at large. 
1. The breakthrough that machine learning has come to represent is therefore not just the 'automation of statistics', as machine learning is sometimes described, but the *automation of automation*, bringing this process to the scale of collective knowledge and cultural heritage. 
1. Further, machine learning can be considered as the technical proof of gradual integration of labour automation with social governance. 
1. Emerging out of the imitation of the division of labour and psychometrics, machine learning models have gradually evolved towards an integrated paradigm of governance that corporate data analytics and its vast datacentres well exemplify.  
1. At this point of analysis, it is important to mention that intrinsic limits affect the current form of machine learning. 
1. In a paper published in September 2021, Neil Thompson and other computer scientists argue that the error-correction techniques of deep learning have reached a computational limit and are unable to grow without paying exorbitant costs of energy and hardware resources which not even big corporations could soon afford. 
1. The issue of computational explosion, which cyclically reappears in the history of AI, this time affects artificial neural networks. 
1. These findings, which can be generalised to other algorithms and error- correction techniques, simultaneously prove that the 'intelligence explosion' of AI is a mirage. 
1. As such, when critical theory engages with cartoonish campaigns to discover the 'alien intelligence' hidden in the black box of AI, it often neglects this logical limit. 
1. What scholars perceive as 'alien intelligence' is, more prosaically, the game of statistical correlations at a very large scale. 
1. There is no evidence of a 'singularity' phenomenon in this game of correlations and, given its computational constraints, current AI runs no risk of becoming the malevolent 'superintelligence' of which Oxford scholar Nick Bostrom has warned. 

### Undoing the master algorithm

1. Given the growing size of datasets, the training costs of large models, and the monopoly of the cloud infrastructure that is necessary to host such models by a few companies such as Amazon, Google, and Microsoft (and their Asian counterparts Alibaba and Tencent), it has become evident to everyone that the sovereignty of AI remains a tough affair of geopolitical scale. 
1. Moreover, the confluence of different apparatuses of governance (climate science, global logistics, and even health care) towards the same hardware (cloud computing) and software (machine learning) signals an even stronger trend to monopolisation. 
1. Aside from the notorious issue of power accumulation, the rise of data monopolies points at a phenomenon of technical convergence that is key to this book: the means of labour have become the same ones of its measurement, and, likewise, the means of management and logistics have become the same ones of economic planning.  
1. This became evident also during the recent COVID-19 pandemic, when a large infrastructure for tracking, measuring, and forecasting social behaviours was established. 
1. This infrastructure, unprecedented in the history of health care and biopolitics, however, was not created *ex nihilo* but built upon existing digital platforms that orchestrate most of our social relations. 
1. Particularly during the lockdowns, the same digital medium was used for working, shopping, communicating with family and friends, and eventually health care. 
1. Digital metrics of the social body such as geolocation and other metadata were key for the predictive models of the global contagion, but they have been long in use for tracking labour, logistics, commerce, and education. 
1. Philosophers such as Giorgio Agamben have claimed that this infrastructure extended the state of emergency of the pandemic, while in fact its deployment to health care and biopolitics continues decades of monitoring the economic productivity of the social body which passed unnoticed to many.  
1. The technical convergence of data infrastructures reveals also that contemporary automation is not just about the automation of an individual worker, as in the stereotypical image of the humanoid robot, but about the automation of the factory's masters and managers, as happens in the gig economy platforms. 
1. From the giants of logistics (Amazon, Alibaba, DHL, UPS, etc.) and mobility (Uber, Share Now, Foodora, Deliveroo) to social media (Facebook, TikTok, Twitter) -- platform capitalism is a form of automation that in reality does not replace workers but multiplies and governs them anew. 
1. It is not so much about the automation of labour this time as it is about the automation of management. Under this new form of *algorithmic management*, we are all rendered as *dividual workers* of a vast automaton comprised of global users, 'turkers', carers, drivers, and riders of many sorts. 
1. The debate on the fear that AI fully replaces jobs is misguided: in the so-called platform economy, in reality, algorithms replace management and multiply precarious jobs. 
1. **Although the revenues of the gig economy remain minoritarian in relation to traditional local sectors, by using the same infrastructure worldwide these platforms have established monopoly positions.** 
1. In conclusion, the power of the new 'master' is not about the automation of individual tasks but the management of the social division of labour. 
1. Against Alan Turing's prediction, it was the master, not the worker, that the robot came to replace first.  
1. One wonders what the chance of political intervention in such technologically integrated space would be, and whether the call to 'redesign AI' that grassroots and institutional initiatives advocate for is either reasonable or practicable. 
1. This call should first respond to the more pressing question: How is it possible to 'redesign' large-scale monopolies of data and knowledge? 
1. As big companies such as Amazon, Walmart, and Google have conquered a unique access to the needs and problems of the whole social body, a growing movement is asking not just to make these infrastructures more transparent and accountable but actually to collectivise them as public services (as Fredric Jameson has suggested, among others), or having them replaced by public alternatives (as Nick Srnicek has advocated). 
1. But what would be a different way to design such alternatives?  
1. As this book's theory of automation has suggested, any technology and institutional apparatus, including AI, is a crystallisation of a productive social process. 
1. Problems arise because such crystallisation 'ossifies' and reiterates past structures, hierarchies, and inequalities. 
1. To criticise and deconstruct complex artefacts such as AI monopolies, first we should engage in a meticulous work of *deconnectionism*, undoing -- step by step, file by file, dataset by dataset, piece of metadata by piece of metadata, correlation by correlation, pattern by pattern -- the social and economic fabric that constitutes them in origin. 
1. This work is already being advanced by a new generation of scholars who are dissecting the global production pipeline of AI, especially those who use methods of *action research*. 
1. Notable, among many others, are Lilly Irani's Turkopticon platform, used for 'interrupting worker invisibility' in the gig platform Amazon Mechanical Turk; Adam Harvey's investigation of training datasets for face recognition, which exposed the massive privacy infringements of AI corporations and academic research; or the work of the Politically Mathematics collective from India, who analysed the economic impact of COVID-19 predictive models on the poorest population and reclaimed mathematics as a space of political struggle (see their manifesto quoted at the beginning of this conclusion).  
1. The labour theory of automation is an analytical principle for studying also the new 'eye of the master' which AI monopolies incarnate. 
1. However, precisely because of the emphasis on the labour process and social relations that constitute technical systems, it is also a synthetic and 'sociogenic' principle (to use Frantz Fanon and Sylvia Wynter's programmatic term). 
1. What is at the core of the labour theory of automation is, ultimately, a *practice of social autonomy*. Technologies can be judged, contested, reappropriated, and reinvented only by moving into the matrix of the social relations that originally constituted them. 
1. Alternative technologies should be situated in these social relations, in a way not dissimilar to what cooperative movements have done in the past centuries. 
1. But building alternative algorithms does not mean to make them more ethical. For instance, the proposal to hard-code ethical rules into AI and robots appears highly insufficient and incomplete because it does not directly address the broad political function of automation at their core.
1. What is needed is neither techno-solutionism nor techno-pauperism, but instead a *culture of invention*, design and planning which cares for communities and the collective, and never entirely relinquishes agency and intelligence to automation. 
1. The first step of technopolitics is not technological but political. 
1. It is about emancipating and decolonising, when not abolishing as a whole, the organisation of labour and social relations on which complex technical systems, industrial robots, and social algorithms are based -- specifically their inbuilt wage system, property rights, and identity politics. 
1. New technologies for labour and society can only be based on this political transformation. 
1. It is clear that this process unfolds also by developing not only technical but also political knowledge. 
1. One of the problematic effects of AI on society is its epistemic influence -- the way in which it renders intelligence as machine intelligence and implicitly fosters knowledge as procedural knowledge. 
1. The project of a political epistemology to transcend AI, however, will have to transmute the historical forms of abstract thinking (mathematical, mechanical, algorithmic, and statistical) and integrate them as part of the toolbox of critical thinking itself. 
1. In confronting the epistemology of AI and its regime of knowledge extractivism, a different technical mentality, a collective 'counter- intelligence', has to be learned.

***

