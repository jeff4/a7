---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'GBC Chapter 5'
LastUpdatedDate: 2023-12-28
CreatedDate: 2023-10-28
description: 'Review of pre-2006 ML before DNNs and Deep Belief Networks'
author: 'Jeff'
tags: []
---

***
## Deep Learning (GBC) Chapter 5
* [Link](https://www.deeplearningbook.org) to main Goodfellow, Bengio, Courville textbook site.
* [Direct link](https://www.deeplearningbook.org/contents/ml.html) to PDF copy of Chapter 5: Machine Learning Basics.
* See also [Learning from Data](www.amlbook.com) book by **Abu-Mostafa, Magdon-Ismail, and Lin (aka AML)**. Relevant chapters: 
	* Chapter 2 (training set, test set, generalization, VC dimension, Bias vs. Variance) 
	* Chapter 3 (Linear Model-- better to map to GBC Chapter 6 XOR) 
	* Chapter 4 (Overfitting, Regularization, Weight Decay, and Validation Dataset)
***
### 5.1 Learning Algorithms
* Section 5.1.1 - The Task *T* can be many things: classification, classification with missing errors, regression, transcription, machine translation, structured output, anomaly detection, synthesis/sampling, imputation of missing values, denoising, density estimation *aka* probability mass function estimation.
* Section 5.1.2 - The Performance Measure *P*. Accuracy + Error Rate = 1.0
	* Reserving a **test set** p. 101
* Section 5.1.3 - The Experience *E*
	* Can be broadly categorized into supervised, unsupervised, and reinforcement learning
	* Rather fuzzy boundary between supervised and unsupervised learning at times. 
	* p. 103. Concept of **design matrix** which is just an object to hold many examples at once. Eg., if there are 150 plants each of which has 4 features, then you can hold that in a matrix of 4 columns and 150 rows of plants (the famous Iris leaf dataset)
* 5.1.4 - Example of Linear Regression and trying to reduce the MSE (Mean Squared Error)
	* Not just about y = **wx**, but also with the bias parameter: y = **wx** + b for the 'y-intercept'.
***
### 5.2 Capacity, Overfitting, and Underfitting
* p. 107 'Again we want to reduce the *training error*; so far what we have described is just an optimization problem like in linear regression.'
* '**Critical point** What separates machine learning from vanilla optimization is that we want the *test error* aka the **generalization error** to be reduced as well.
	* 'The definition of generalization error (aka test error) is **the expected value of the error on a *new* input.**
	* 'Here the expectation is taken across different possible inputs drawn from the distribution of inputs we expect the system to encounter in practice.
* p. 108 here is where we show why it's important there is a general similarity between the examples in the training dataset and the test data set. We must use the **i.i.d.** assumptions; the examples are *independent* of each other in the same way scross both training and test data sets; *and* the examples are *identically distributed* aka drawn from the exact same probability distribution
* Another way of expressing this is that the examples in both the training and test data sets are created by a *data-generating process*. And if the probability distribution of output examples is identical across both training and test sets, then for our purposes, we can statistically treat these two sets as being generated by the same data-gen process.
	* We call the underlying distribution the **data-generating distribution** denoted *p*<sub>data</sub>.
* The probabilisitic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error (aka between training error and generalization error).
* If we are *not* engaging in ML, then given the assumptions above, we training error = test error. However, in ML, *when we use an ML algorithm, we do **not** fix the parameters ahead of time.* First, we sample the training set, then we use that to choose the parameters to reduce the training set error iteratively. Only then do we sample from the test data set. 
	* Under this process, the expected test error is greater than or equal to the expected value of traiing error.
* The factors determining how well a machine learning algorithm will perform are its ability to:
	1. Make the training error *small*
	1. Make the gap between training error and test error (generalization error) *small*
* More on factors
	* Factor one: make the training error too large = underfitting
	* Factor two: make the gap between training and test error too large = overfitting
#### Model Capacity
* p. 109 We can control whether a model is more likely to overfit or underfit by altering the **capacity**. Loosely defined, a model's capacity is its ability to fit a wide variety of functions.
	* Models with low capacity may struggle to fit the training set.
	* Models with high capacity can overfit by memorizing properties of the training set that do not serve them well in the test set, thereby having higher than desired generatlization error.
* One way ot control the capacity of a learning algorithm is by choosing its **hypothesis space**. 
	* i.e., the set of functions that the learning algorithm is allowed to select as being the solution.
	* e.g., the linear regression algorithm has as its hypothesis space the set of all linear functions. To increase this model's capacity, we can expand the hypothesis space to include all polynomial functions as well. 
* Choice of model / hypothesis space is not the only way to expand capacity. Note the difference between **effective capacity** and **representational capacity** as shown between p. 110 and 111.
* p.111 VC dimension aka **Vapnik-Chervonenkis dimension** measures the capacity of a binary classifer.
* p.112 chart on difference between Training Error and Generalization Error. 
##### Non-parametric models
* p.111-112 To reach the most extreme case of arbitrarily high capacity, we introduce the concept of **non-parametric models**.
	* So far we have only seen **parametric models** such as linear regression.
	* Parametric models learn functions that can be expressed by a vector of parameters whose size is *fixed* and *finite* **before any data is observed.**
	* In contrast, a non-parametric model need not have a vector of fixed size before data observation
* Sometimes, non-parametric models are just theoretical abstractions that cannot be implemented in practice. e.g., an algorith that searches over all possible probability distributions.
* However, we can also design practical nonparametric models by making their complexity a function of the training set size.
	* one example: **nearest neighbor regression**. Unlike linear regression, which has a fixed-length vector of weights, the nearest neighbor regression model simply stores the **X** and y&#8407; from the training set. *
		* When asked to classify a test point x&#8407; the model looks up the nearest entry in the training set and returns the associated regression target.
		* In other words, y-hat = y<sub>i</sub> where i = arg min (*L<sup>2</sup>* norm).
	* the nearest neighbor regression algorithm above can be generalized to distance metrics other than the *L<sup>2</sup>* norm. e.g., learned distance metrics--See Goldberger et al 2005.
* Finally, we can create a non-parametric learning algorithm by wrapping a parametric learning algorithm inside another algorithm that increases the number of parameters needed.
	* e.g., we could imagine an outer loop of learning that changes the degree of the polynomial learned by linear regression on top of a polynomial expansion o the input.
* p.113 The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on many problems because there may be some noise in the distribution.
* p. 113 Training and generalization error vary as the size of the training set varies.
	* Expected generalization error can never increase as the number of training examples increases.
* For non-parametric models, more data yield better generalization until the best possible error is achieved.
* Any fixed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes Error. (From earlier on this page [p.113], *Bayes error* = the error incurred by an oracle making predictions from the true distribution p(**x**,y).)
* Note: it is possible for the model to have optimal capacity and yet still have a large gap between training and generalization errors. In this case, we may be able to reduce the gap by adding more examples to the training dataset.


### 5.2.1 The No Free Lunch Theorem
* p.113 Wolpert 1996 proved the 'no free lunch theorem for machine learning' which states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifyig previously unobserved points.
	* i.e., no ML algorithm is universally better than any other.
	* The most sophisticated algorithm we can concieve of us has the same avg performance (over all possible tasks) as merely predicting that every point belongs to the same class.
* p.115 Fortunately, these results hold only when we average over *all* possible data-generating distributions. If we make some assumptions about the kinds of probability distributions we encounter in the real world, then we can design learning algorithms that peform well on those particular distributions.
*  This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the “real world” that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data-generating distributions we care about.

### 5.2.2 Regularization
* p.115 The only method for modifying a learning algorithm discussed so far is by increasing or decreasing a model's representational capacity by increasing or decreasing the size of the hypothesis space from which we choose the "best" function. i.e., by adding or removing more and fewer functions from the set of possible solutions.
* However, the representational capacity of a model can be changed not only by the raw size of the hypothesis space, but also by the weighting or preference we give to certain types of functions contained in that space.
* For example, we can put our thumb on the scale such that our ML algorithm 'prefers' polynomial functions versus linear functions. In that case, if a polynomial and a linear function are both eligible to solve our problem, the linear function will be chosen only if the linear function fits the training data significantly more than the closest polynomial function does.
* Example of using **weight decay**. Assume we want to minimize the objective function J(w&#8407;) where J() = Mean Squared Error for training + preference for weights to have a smaller L<sup>2</sup> norm. See Equation 5.18 on p.116. 
	* When &#955; = 0, we impose no preference. 
	* As &#955; gets larger, it forces weights to become smaller.
* More generally, we can regularize a model that learns a function f(**x**&#8407;;**&#952;**) by adding a penalty called a **regularizer** to the cost function. In the case of weight decay above, the regularizer is omega(w&#8407;) = w&#8407;<sup>T</sup>w&#8407;. Essentially everything except for the &#955; lambda scalar multiplier.
* Expressing preferences for one function over another is a 'gentler' more general way of controlling a model's capacity than 'violently' including or excluding functions from a hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.
* p.117 The philosophy of deep learning is that for a wide range of tasks–and indeed all intellectual tasks that people can do–may all be solved effectively using very general-purpose forms of regularization.

***
### 5.3 Hyperparameters and Validation Size
* Most ML algos have hyperparamters aka settings that we use to control the algorithm's behavior. The values of hyperparameters are *not* adjusted via the learning algorithm itself.
* Of course, we can have a nested learning procedure whereby one ML algorithm learns the best hyperparameters for another ML algorithm.
* The polynomial regression example in Figure 5.2 on p.110 has as single hyperparamter: the degree of the polynomial which acts as a capacity hyperparamter.
* p.118 Sometimes a setting is considered a hyperparameter because it is just too difficult to optimize; i.e., the learning algorithm doesn't have a path to improve it over time. 
* More frequently, a setting is considered a hyperparameter b/c it's not appropriate to learn that hyperparameter in the training set. 
	* This applies to all hyperparameters that control model capacity.
	* If a setting that managed model capacity was learned on the training set, **that setting would always choose the maximum possible model capacity** aka always resulting in overfitting.
	* For example, we can always ﬁt the training set better with a higher-degree polynomial and a weight decay setting of λ= 0 than we could witha lower-degree polynomial and a positive weight decay setting.

#### Validation Set
* This helps solve the problem of overfitting.
* Before, we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. 
* It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. 
* **For this reason, no example from the test set can be used in the validation set.** Therefore, we always construct the validation set from the *training* data. 
	* i.e., **these 3 datasets are unique and disjoint: (1) training set, (2) validation set, and (3) test set.**
* Speciﬁcally, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. 
* The other subset is our validation set, used to estimate the generalization error during or after training,allowing for the hyperparameters to be updated accordingly. The subset of dataused to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. 
* The subset of data used to guide the selection of hyperparameters is called the validation set. 
* Typically, one uses about 80 percent of the training data for training and 20 percent for validation. 
* Since the validation set is used to “train” the hyperparameters, the validation set error will underestimate thegeneralization error, though typically by a smaller amount than the training errordoes. 
* After all hyperparameter optimization is complete, the generalization errormay be estimated using the test set.
* JH: Note that validation set is a subset of the original training set. e.g., if there are 100 examples in the original training set pre-segregation, we might reserve 80 examples for the post-segregration new training set for actual training. And the remaining 20 examples will be used for the validation set.
* p.118 In practice, when the same test set has been used repeatedly to evaluate performance of different algorithms over many years, and especially if we consider all the attempts from the scientiﬁc community at beating the reported state-of-the-art performance on that test set, we end up having optimistic evaluations with the test set as well. 
* Benchmarks can thus become stale and then do not reﬂect the true ﬁeld performance of a trained system. Thankfully, the community tends to move on to new (and usually more ambitious and larger) benchmark datasets.

#### 5.3.1 Cross-validation
* If your original dataset has 100,000s of examples, don't really worry about how you split it into training, validation, and test datasets.
* However, if your original pool of data is too small, it's hard to prove good generalizability. This is when alternative procedures let one use all the examples in the estimation of the mean test error at the price of increased computational cost.
	* These procedures are based on the idea of repeating the training and test computation on different randomely chosen subsets or splits in the original dataset.
	* The most common of these is the *k*-fold cross-valdiation procedure shown on p.120 (Algorithm 5.1) formed by *k* number of non-overlapping subsets.
	* This solution is somewhat imperfect see Bengio and Grandvalet (2004), but approximations are still used.

***
### 5.4 Estimators, Bias, and Variance
* p.118 The field of statistics provides many tools to not only improve a task during training but to let that algorithm generalize. Let's consider parameter estimation, bias, and variance.

#### 5.4.1 Point Estimation
* *Point estimation* is the attempt to provide the single 'best' prediction of some quantity of interest. In general, the quantity of interest can be a single parameter of a vector of parameters in some parametric model such as the weights in our linear regression example in section 5.1.4., but it can also be a whole function.
* A parameter **&#952;** can have a 'point estimate version' denoted with a hat on top aka theta-hat. Just as a unit vector v with a hat on top is called a 'v-hat'.
* Let { x&#8407;<sub>1</sub>,  x&#8407;<sub>2</sub>,  x&#8407;<sub>3</sub>, ...  x&#8407;<sub>m</sub>} be a set of *m* i.i.d. data points. A *point estimator* is any function `g()` that can take in that list of vectors as input and returns a value **&#952;-hat** 
* While almost any function qualifies as a point estimator based on the above definition, we are looking for *good* estimators. In that case, we want the output **&#952;-hat** to be close to the true underlying **&#952;**.So only estimator functions that output a reasonable **&#952;-hat** will be considered for our desired point estimator function.

#### Function Estimators
* Distinguised from point estimators above because we are not trying to just estimate a single point, but an entire function.

#### 5.4.2 Bias
#### 5.4.3 Variance aka Standard Error
* **Variance** = Var(&#952;-hat) 
* Difference betwen **Bias** from previous section 5.4.2 and **Variance** in this section.
* &#8730;<span style="text-decoration: overline; text-decoration-thickness: 1.5px"> Variance </span>= *Standard Error* = *SE(&#952;-hat)* 
    * i.e., *SE<sup>2</sup>* = *SE* * *SE* = the square of *Standard Error* = Variance
* Explaining *SE<sup>2</sup>* = Variance via Bernoulli Distribution p. 125
#### 5.4.4 Minimizing Mean Square Error by trading off between Bias and Variance
* Bias and variance measure two different sources of error in an estimator.
	* Bias measures the expected deviation from teh true value of the function or parameter.
	* OTOH, variance provides a measure of the deviation from the expected estimator value *caused specifically* by any particular sampling of the dataset.
* How do we choose between the tradeoff between bias and variance? See the total generalization error chart on p.127. GenError = BiasError + VarianceError. We can see that Bias steadily decreases as we increase model capacity. By the same token, Variance monotonically increases with model capacity.
* Therefore, there is a minimum somwhere that is the optimal capacity where the sum of BiasError + VarianceError is the lowest possible at least wrt model capacity 
* The most common way to negotiatn the tradeoff between Bias and Variance is to use cross-validation.
	* Empirically, *cross-validation is highly successful in many real-world tasks*. 
	* Alternatively, we can also compare the MSE (mean squared error) of the estimates. See equations 5.53 and 5.54 on p.126. b/c MSE = Bias<sup>2</sup> + Var, that means that when Var=0, MSE = Bias<sup>2</sup>. Conversely, when Bias<sup>2</sup>=0, then MSE = Var.
* **Desirable estimators are those with a small MSE**. These are estimators that manage have small overall MSE; i.e., these are estimators that manage to keep both their bias and variance in check.

#### 5.4.5 Consistency
* So far, we have only discussed the properties of estimators based on varying model capacity but maintaining the size of the training data set. i.e., fixed training set size.
* What happens to our estimator when we increase the amt of training data?
* Recall that the number of data examples = m. As m approaches infinity, we want to design a system st the estimator theta-hat converges on the true value of the underlying datagenerating process theta.
* The more the above bullet is true, the higher the **consistency**.
* i.e., If we are very certain that theta-hat converges on theta as m = number of examples grow, then we say there is *strong consistency*.
* i.e., If we are pretty *uncertain* that theta-hat converges on theta as m = number of examples grow, then we say there is *weak consistency*.
* Consistency ensures that the bias induced by the estimator diminishes as the number of data exaples grows.
	* However, *the converse is not true*; asymptotic unbiasedness does *not* imply consistency.

***
### 5.5 Maximum Likelihood Estimation
* How do we choose estimators? We can randomly pick functions and evaluate their MSE, bias, variance, consistency. But is there are more rational way of picking our initial estimator from the beginning??
* The most common principle is the **Maximum Likelihood Principle**.
* Review how squaring the maximum likelihood estimator for theta makes a simpler addition sum as opposed to a multiplicative product which is easier to manipulate. (Capital Sigma for sums is better than Capital Pi for products.) p. 128
* So what is MaxLikelihood really? One interpretation is to view it as minimizing the dissimilarity between the empericial distribution p-hat<sub>data</sub> defined by the training set and model distribution **versus** the degree of dissimilarity between training set and model distribution  as measured by KL divergence.
	* For more on **KL divergence** aka **Kullbeck-Leibler divergence**, see p.72-73 in *Section 3.13 Information Theory*. (Chapter 3 of GBC a review of Probability and Info Theory.)
* p.129 Minimizing KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Many authors use the term 'cross-entropy' refer to the negative log-likelihood of a Bernoulli or softmax distribution, *but this is a misnomer*. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distributino defined by the training set and the probablity distribution defined by model.
	* e.g., mean square error (MSE) is the cross-entropy between the empirical distribution and the Gaussian model. 
* In other words, MaxLikelihood is an attempt to make the model distribution match the empirical distribution p-hat<sub>data</sub>
* While the optimal **&#952;** is the same regardless of whether we are **maximizing the likelihood** *or* **minimizing the KL divergence**, the values of the objective functions are different.
	* In software, we often phrase both *maximizing the likelihood* and *minimizing  the KL* as `minimizing the cost function` **J**&#8407; 
* Given the above terminology, Maximum Likelihood therefore becomes minimization of the **negative log-likelihood (NLL)**, aka minimization of cross-entropy. 
	* The perspective of maximum likelihood as minimum KL divergence becomes helpful in this ase because the KL divergence has a known minimum value of zero.
	* The NLL however *can* become negative when x&#8407; is real-valued. 
#### 5.5.1. Conditional Log-Likelihood and MSE p.129
* Note that for most supervised learning (aka training with labelled data), the maximum likelihood estimator can be easily generalized to estimate the conditional probabilty that we are looking at the correct output vector y&#8407;, given:
	1. an input vector x&#8407; 
	1. the underlying data-generating parameter **&#952;** 
		* (See p. 119 section 5.4.1 to see initial introduction of parameter **&#952;** and how it is estimated by the point estimator **&#952;-hat**.)
* See bottom of p.129 for decomposition of arg max, and p.130 for an example of how linear regression can be shown to be a maximum likelihood procedure.
#### 5.5.2 Properties of Maximum Likelihood
* p.131 The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically. i.e., the limit as the number of training examples *m* increases to infinity, the more likely the estimator **&#952;-hat<sub>m</sub>** is to converge to the 'true value' of the platonic ideal of the parameter **&#952;**. (see p.127 again)
* Consistency (see again section 5.4.5 p.126-127)
* Two required conditions for consistency:
	1. The true distribution p<sub>data</data> must lie within the model family p<sub>model</sub>(**&#952;**)
	1. The true distribution p<sub>data</data> must correspond to *one and only one* value of **&#952;**. Otherwise, it will be impossible for the maximum likelihood to recover the correct p<sub>data</data> b/c it cannot distinguish which particular value of **&#952;** was used by the data-generating process.
* Different estimators can all have the property of consistency *but* still differ in their **statistical efficiency**.
	* The degree of *statiistical efficiency* is often studied in the **parametric case** focused on the value of a parmeter only, not on the value of an entire function.
* Thus, for reasons of *consistency* and *efficiency*, maximum likelihood is often considered the preferred estimator to use for machine learning.

***
### 5.6 Bayesian Statistics
* p.132 to 136
* p. 5.6.1 **Maximum A Posteriori** Estimation *aka* **MAP Estimation** p. 135


***
### 5.7 Supervised Learning Algorithms
* p.137 general category of **probability of output y** *p(y)* given the input of input vector x&#8407; as well as the best parameter vector **&#952;** for a parametric family of distributions.
	* Specific example of how linear regression fits into the above family.
	* For *k* number of classes, the sum of all the classes' probability needs to add up to 1. For example, assume that matter can only take on 3 phases: solid, liquid, or gas. Then p(solid) + p(liquid) + p(gas) = 1 for a given substance when we are trying to figure out what phase/class a particular compound belongs in given standard temperature and pressure. In this case, k = 3.
* For a normal Gaussian distribution over real-valued numbers, we parameterize it with the mean value.
* For a distribution over a binary variable, we have the "squash" the outcome to be between 0 and 1.
	* One way to do this is to use the logistic sigmoid function. This is called *logistic regression*
	* There is no closed-form solution to finding the optimal weights of a logistic regression. (we can't simply solve the system of equations the way we could in linear regression.) Instead, for logistic regression, we try to maximize the log-likelihood. 
	* Another way of saying maximize the log-likelihood is to minimize the NLL (negative log-likelihood) using gradient descent.

#### 5.7.2 Support Vector Machines
* p.137 One of the most influential approaches to supervised learning is the Support Vector Machine (SVM). See papers from 1992 and 1995 by Boser, Cortes, Vapnik.
* Similar to logistic regression; BUT different in that it does not output probabilities. Instead, it outputs a class identity.
* How the kernel trick works. Basically rewrite ML algorithm into dot product / inner products. p.138.
* Two benefits of the kernel trick:
	1. Lets ML algortihms learn models that are nonlinear as a function of x&#8407; using convex optimization techniques that are guaranteed to converge.
	1. The kernel function *k* often admits an implementation that is significantly more computationally efficient than naive base case. 
* The most commonly used kernel is the **Gaussian kernel**. p.139
#### Kernel machines p. 139
* SVMs introduced **the kernel trick** which can be applied to other linear models (not just support vector machines). 
	*. p.139 The category of algorithms that employ the kernel trick is known as **kernel machines** that use **kernel methods**. See Williams and Rasmussen (1996) and Schölkopf et al (1999) for more.
* A major drawback to kernel machines is that the cost of evaluating the decision function is linear with the *m* the number of training examples. SVM mitigate this by only learning an alpha vector that contains mostly zeros. Thus classifying a new example only requires evaluating the kernel function for training examples that have a non-zero alpha variable instance. These training examples are called **support vectors**. 
* Kernel machines also suffer from a high computational cost of training when the dataset is large. Kernel machines with generic kernels struggle to generalize well. For more, see sections 5.9 and 5.11.
* Kernel machines are really important to the history of deep learning. The modern incarnation of DL was designed to overcome the above limitations of kernel machines.
* In particular, the modern deep learning renaissance began when Hinton et al (2006) demonstrated that an ANN could outperform the RBF kernel SVM on the MNIST handwriting benchmark. (RBF = radial basis function at the beginning of p.139)

#### 5.7.3 Other Simple Supervised Learning Algorithms
* p.139-142 KNN = K-Nearest Neighbors and Decision-Trees.
* p.142 KNN and decision trees have many limitations *but* they are useful learning algorithms when computational resources are constrained.
* See Murphy *Machine Learning: A Probabilistic Perspective* (2012); Bishop *Pattern Recognition and Machine Learning* (2006); and *Introduction to Statistical Learning* by James, Witten, Hastie, and R. Tibshirani (2021) as good ML textbooks to learn more about traditional supervised learning algorithms.


***
### 5.8 Unsupervised Learning Algorithms
* add as needed

#### 5.8.1 Principal Components Analysis
* PCA
* add as needed

#### 5.8.2 k-means Clustering
* add as needed


***
### 5.9 Stochastic Gradient Descent
* Intro to SGD. Calls back to basic gradient descent algorithm from Section 4.3 (p.79) and deeper dive in Chapter 8.

***
### 5.10 Building an ML Algoirthm
* Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe where we combine:
	1. dataset
	1. cost function
	1. optimization procedure
	1. a model
* For example, the linear regression algorithm combines:
	1. Dataset of **X**&#8407; and y&#8407; 
	1. cost function = J(w&#8407;, b) = -Estimate<sub>x,y~p<sub>data</sub></sub>log p<sub>model</sub>(y | x)
	1. optimization procedure = solving for where the gradient of the cost function J=0 using the normal equations
	1. model specification = p<sub>model</sub>(y|x) = N(y;**x**&#8407;-transposed**w** + b, 1) 
* By realizing that we can replace any of these components mostly independently from the others, we can obtain a wide range of algorithms.
* The cost function typically includes at least one term that causes the learning process to perform statistical estimation.
	* The most common cost function is the NLL which causes the maximum likelihood estimation.
* The cost function often also includes some regularization terms. e.g., a weight decay term.
* If we cahgne the model from linear to non-linear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical optimization procedure such as gradient descent.
* In some cases, the cost function may be a function that we cannot actually evaluate, for computational reasons. In these cases, we can still approximately minimize it using iterative numerical optimization, as long as we have some way of approximating its gradients.
* Most machine learning algorithms make use of this recipe, though it may not be immediately obvious. If a machine learning algorithm seems especially unique or hand designed, it can usually be understood as using a special-case optimizer.
* Some models, such as decision trees andk-means, require special-case optimizers because their cost functions have ﬂat regions that make them inappropriate for minimization by gradient-based optimizers.
* Recognizing that most machine learning algorithms can be described using this recipe helps to see the diﬀerent algorithms as part of a taxonomy of methods for doing related tasks that work for similar reasons, rather than as a long list of algorithms that each have separate justiﬁcations.

***
### 5.11 Challenges that motivated deep learning
* The simple machine learning algorithms described in this chapter work well on a wide variety of important problems. They have not succeeded, however, in solving the **central problems in AI**, e.g., **(1) recognizing speech** or **(2) recognizing objects.**
* The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on such AI tasks.
* This section is about how the challenge of generalizing to new examples becomes exponentially more diﬃcult when working with high-dimensional data, and how the mechanisms used to achieve generalization in traditional machine learning are insuﬃcient to learn complicated functions in high-dimensional spaces. Such spaces also often impose high computational costs. Deep learning was designed to overcome these and other obstacles.

#### 5.11.1 The Curse of Dimensionaliy
* Diagram of 1d, 2d, and 3d and associated stastical challenge
* The more complex aka the higher the number of dimensions the input space has, the harder it is to have enough training examples to capture the true underlying probability distribution.
	* Consider the 1d version of the input space. When we try to generalize to a new datapoint within that input space, we can usually tell what to do simpyly by inspecting the training examples that already lie in that 1-d cell; presumably, there are enough examples within that cell or nearby to give us a good probability density.
	* Here are some examples:
		* **Classification task:** Return the most common class of training examples in that cell (e.g., 'dog' vs. 'cat').
		* **Estimating the probability density task:** Estimating the probability density at location x&#8407; just requires us to retrain a fraction. Numerator = number of training examples in that grid cell. Denominator = total number of training in the entire training data set.  
		* **Regression task:** Average the target values observed over the examples within that grid cell.
	* But what about when we have high-dimensional spaces; not just 3d, but n-dimensional? In these cases, the number of grid locations is massive--much larger than the number of examples in our training dataset.
	* Many traditional ML algorithms simply assume that th eoutput of a new point should be approximately teh same as the output at the nearest training point.

#### 5.11.2 Local Constancy and Smoothness Regularization
* To generalize well, ML algorithms need to be guided by prior beliefs about what kind of function they should learn. 
* We have seen these priors incorporated as explicit beliefs in the form of probability distributions over parameters of the model. 
* More informally, we may also discuss prior beliefs as directly inﬂuencing the function itself and inﬂuencing the parameters only indirectly, as a result of the relationship between the parameters and the function. 
* Additionally, we informally discuss prior beliefs as being expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another, even though these biases may not be expressed (or even be possible to express) in terms of aprobability distribution representing our degree of belief in various functions.
* Among the most widely used of these implicit *priors* is **the smoothness prior** *aka* **the local constancy prior**.
	* This prior assumes that the function we are learning does not change within a small region. (like manifolds?)
	* We explain below why the smoothness prior by itself is insufficient for many ML goals.
* Refer to Equation 5.103 (p.154). f<sup>\*</sup>(x&#8407;) &#8776; f<sup>\*</sup>(x&#8407; + **&#949;**)
* Eq. 5.103 shows a function that is smooth aka locally constant, for most configurations of x&#8407; and a small change **&#949;**. In other words, if we know a good answer for input x&#8407; then that answer is probably good in the neighborhood of x&#8407; (e.g., if x&#8407; is a labelled training example). 
* If we have several good answers in some neighborhood, we would combine them (by some form of averaging or interpolation) to produce an answer that agrees with as many of them as much as possible.
* An extreme example of the local constancy approach is the *k*-nearest neighbors family of learning algorithms. These predictors are literally constant over each region containing all the points x&#8407;  that have the same set of *k*-nearest neighbors in the training set. For *k* = 1, the number of distinguishable regions cannot be more than the number of training examples.
* While the *k*-nearest neighbors algorithm copies the output from nearby training examples, most kernel machines interpolate between training set outputs associated with nearby training examples.
* An important class of kernels is the family of **local kernels**, where **k (** u&#8407; , v&#8407; **)** has a large value when  u&#8407; and v&#8407; are similar or equal. But the further away u&#8407; and v&#8407; are from each other, the lower the value of k(). 
* A local kernel can be thought of as a similarity function that performs *template matching*, by measuring how closely a test example x&#8407; resembles each training example **x&#8407; <sup>(i)</sup>**. 
* Much of the modern motivation for deeplearning is derived from studying the limitations of local template matching andhow deep models are able to succeed in cases where local template matching fails. See Bengio, Delalleau, and Le Roux 2006 'The curse of highly variable functions for local kernel machines.' 

#### Decision trees and local constancy / smoothness regularization
* p.153. Decision trees also suﬀer from the limitations of exclusively smoothness-based learning, because they break the input space into as many regions as there are leaves and use a separate parameter (or sometimes many parameters for extensions of decision trees) in each region. 
* p.154 If the target function requires a tree with at least *n* leaves to be represented accurately, then at least *n* training examples are required to ﬁt the tree. A multiple of *n* is needed to achieve some level of statistical conﬁdence in the predicted output. 
* In general, to distinguish `O(k)` regions in input space, all the above methods require `O(k)` examples. Typically, there are `O(k)` paramters, with `O(1)` parameters associated with each of the `O(k)` regions. The nearest neighbor scenario, in which each training example can be used to define at most region. See Fig 5.10 on p.154.
* Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples? Clearly, assuming only smoothness of the underlying function will not allow a learner to do that. 
	* For example, imagine that the target function is a kind of checkerboard. A checkerboardcontains many variations, but there is a simple structure to them. 
	* Imagine what happens when the number of training examples is substantially smaller than the number of black and white squares on the checkerboard. Based on only local generalization and the smoothness or local constancy prior, the learner would be guaranteed to correctly guess the color of a new point if it lay within the same checkerboard square as a training example. 
	* There is no guarantee, however, that the learner could correctly extend the checkerboard pattern to points lying in squares that do not contain training examples. 
	* With this prior alone, the only information that an example tells us is the color of its square, and the only way to get the colors of the entire checkerboard right is to cover each of its cells with at least one example.
* p.155 The smoothness assumption and the associated nonparametric learning algorithms work extremely well as long as there are enough examples for the learning algorithm to observe high points on most peaks and low points on most valleys of the true underlying function to be learned. 
* This is generally true when the function to be learned is smooth enough and varies in few enough dimensions. In high dimensions, even a very smooth function can change smoothly but in a diﬀerent way along each dimension. 
* If the function additionally behaves diﬀerently in various regions, it can become extremely complicated to describe with a set oftraining examples. If the function is complicated--we want to distinguish a huge number of regions compared to the number of examples--is there any hope to generalize well?
* The answer to both of these questions--whether it is possible to represent a complicated function eﬃciently, and whether it is possible for the estimated function to generalize well to new inputs--is yes.
* The key insight is that a very large number of regions, such as *O(2<sup>k</sup>)*, can be defined with *O(k)* examples, so long as we introduce some dependencies between the regions through additional assumptions about the underlying data-generating distribution.
* In this way, we can actually generalize nonlocally. See Bengio and Monperrus (2005) and Bengio, Larochelle, and Vincent (2006) 'Non-local manifold Parzen windows'
* Many different deep learning algorithms provide implicit or explicit assumptions that are reasonable for a broad ranger of AI tasks in order to capture these advantages.
* p.155 Other approaches to machine learning often make stronger, task-specific assumptions.
	* For example, we could easily solve the checkerboard task by providing the assumptino that the target function is periodic.
	* Usually we do not include such strong, task-specific assumptions in neural networks so that they can generalize to a much wider variety of structures.
* AI tasks have structure that is much too complex to be limited to simple, manually specified properties such as periodicity, so we want learning algorithms that embody more general-purpose assumptions. 
* The core idea in deep learning is that we assume that the data was generated by the *composition of factors*, or features, potentially at multiple levels in a hierarchy.
* Many other similarly generic assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and teh number of regions that can be distinguished.
* We describe these exponential gains more precisely in sections 6.4.1, 15.4, and 15.5. 
* The exponential advantages conferred by the use of deep distributed representations counter the exponential challenges posed by the curse of dimensionality.

#### 5.11.3 Manifold Learning
* p.156 An important concept underlying many ideas in ML is that of a **manifold**.
* A manifold is a connected region. Mathematically, it is a set of points associated with a neighborhood around each point. From any given point, the manifold locally appears to be a Euclidean space. 
	* e.g., In everyday life, we experience the surface of the world as a 2-D plane, but it is in fact a spherical manifold in3-D space.
* The concept of a neighborhood surrounding each point implies the existence of transformations that can be applied to move on the manifold from one position to a neighboring one. 
	* In the example of the world’s surface as a manifold, one can walk north, south, east, or west.
* Although there is a formal mathematical meaning to the term “manifold,” in machine learning it tends to be used more loosely to designate a connected set of points that can be approximated well by considering only a small number of degrees of freedom aka dimensions embedded in a higher-dimensional space.
* Each dimension corresponds to a local direction of variation. 
	* See ﬁgure 5.11 for an example of training data lying near a one-dimensional manifold embedded in two-dimensional space. * In the context of machine learning, we allow the dimensionality of the manifold to vary from one point to another. This often happens when a manifold intersects itself. 
	* For example, a ﬁgure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center.
* Many ml problems seem hopeless if we expect the algorithm to learn functions with interesting variations across all of &#8477;<sup>n</sup>. *But...*
* *...**Manifold Learning** algorithms surmount this obstacle!* 
* These algorithms succeed by assuming that most of &#8477;<sup>n</sup> consists of invalid inputs, and that interesting inputs occur only along a collection of manifolds containing a small subset of points--with interesting variations in the output of the learned function occurring only along directions that lie on the manifold, or with interesting variations happening only when we move from manifold 1 to manifold 2, etc.
* Manifold Learning was introduced in the case of continuous-valued data and in the unsupervised learning setting. However, this idea has been generalized to both discrete data and in supervised learning settings.
	* p.157 The key point for effective manifold learning is that the probability mass is highly concentrated.
* The assumption that data lies along a low-dimensional manifold may not always be correct or useful. However, GBC argue that in the context of AI tasks--involving processing images, sounds, text--the manifold assumption is at least a good starting point or approximately correct.

##### Two reasons that suggest that the manifold hypothesis (referred to as the manifold assumption above) is reasonable

1. The first observation in favor of the **manifold hypothesis** is that the probability distribution over images, text strings, and sounds that occur in real life is highly concentrated.
	* Uniform noise essentially never resembles structured inputs from these domains. Figure 5.12 (p.158) shows how uniformly sampled points look like the patterns of static that appear on analog television sets when no signal is available. 
	* Similarly, if you generate a document by picking letters uniformly at random, what is the probability that you will get a meaningful English-language text? Almost zero!... 
	* ...because most of the long sequences of letters do not correspond to a natural language sequence: the distribution of natural language sequences occupies a very little volume in the total space of sequences of letters.  
	* Of course, concentrated probability distributions are not suﬃcient to show that the data lies on a reasonably small number of manifolds. 
	* We must also establish that the examples we encounter are connected to each other by other examples--with each example surrounded by other highly similar examples that can be reached by applying transformations to traverse the manifold. 
1. The second argument in favor of the manifold hypothesis is that we can imagine such neighborhoods and transformations, at least informally. 
	* In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects, and so forth. 
	* Multiple manifolds are likely involved in most applications. For example,the manifold of human face images may not be connected to the manifold of catface images.
	* These thought experiments convey some intuitive reasons supporting the manifold hypothesis.
	* More rigorous experiments clearly support the hypothesis for a large class of datasets of interest in AI. See list of papers on p.159 from 1998-2010.

##### Conclusion to Manifold Learning section
* When the data lies on a low-dimensional manifold, it can be most natural for ML algorithms to represent the data in terms of coordinates on the manifold, rather than in terms of coordinates in &#8477;<sup>n</sup>. 
* In everyday life, we can think of roads as 1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. 
* Extracting these manifold coordinates is challenging but holds the promise of improving many machine learning algorithms. 
* This general principle is applied in many contexts. 
* Figure 5.13 shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure 20.6, we will see how a machine learning algorithm can successfully accomplish this goal.
***
### 5.12 Conclusion
* Now that Chapter 5 is finished, let's turn to **Chapter 6: FF Networks**, which is when we will begin the study of deep learning proper.



***


##### Double-struck R for Real numbers
* "rp = double-struck R = &#8477; 
* "tp = theta = **&#952;**
* "hp = theta-hat = **&#952;-hat**
* "pp = vector arrow above prior character = &#8407; 


